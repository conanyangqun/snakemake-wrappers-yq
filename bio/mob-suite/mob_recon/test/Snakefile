# mob_recon test case
# This is a mini-workflow, can be called from other working directory.
# Usage: 
# snakemake --cores -s /xxx/Snakefile --config fasta=xxx s_id=xxx sample_info=xxx out_dir=xxx
# --wrapper-prefix file:///xxx/snakemake-wrappers-yq/

import csv
import os

# Get parameters from config
fasta = config.get('fasta', None)
sample_id = config.get('s_id', "test")
sample_info = config.get('sample_info', None) # tsv file, with s_id, fasta
out_dir = config.get('out_dir', os.getcwd())

# Samples dictionary
samples = {} # s_id: fasta
if fasta or sample_info:
    if fasta:
        samples[sample_id] = fasta
    elif sample_info:
        with open(sample_info, 'r') as f:
            reader = csv.DictReader(f, delimiter='\t')
            for row in reader:
                samples[row['s_id']] = row['fasta']
    else:
        raise ValueError(f"fasta or sample_info must be provided, but got {fasta} and {sample_info}")
else:
    # Demo case
    samples['demo'] = os.path.join(out_dir, "demo.fasta")

# Rules
rule all:
    input:
        contig_reports = [os.path.join(out_dir, s_id, "contig_report.tsv") for s_id in samples.keys()]

rule download_demo:
    output:
        demo_fasta = os.path.join(out_dir, "demo.fasta")
    shell:
        """
        FASTA_URL='https://github.com/phac-nml/mob-suite/raw/refs/heads/master/mob_suite/example/assembly_no_biomarkers.fasta'
        
        # Download FASTA file
        if command -v wget >/dev/null 2>&1; then
            wget -O demo.fasta ${{FASTA_URL}}
        elif command -v curl >/dev/null 2>&1; then
            curl -L -o demo.fasta ${{FASTA_URL}}
        else
            echo "wget or curl is required to download demo data"
            exit 1
        fi
        
        mv demo.fasta {output.demo_fasta}
        """

rule run_mob_recon:
    input:
        fasta = lambda wc: samples[wc.s_id]
    output:
        contig_report = os.path.join(out_dir, "{s_id}", "contig_report.tsv")
    threads: 4
    params:
        extras = "",
    wrapper:
        "bio/mob-suite/mob_recon"
